import tensorflow as tf
import pandas as pd
from torch.nn.init import xavier_normal_
import numpy as np
import pickle
import argparse

from utils.embedding import CustomEmbeddingModel


# function for extracting best parameters from an optuna trials data frame
def extract_best_params(df, score_col='user_attrs_both.realistic.adjusted_arithmetic_mean_rank_index'):
    sorted_scores = df.sort_values(score_col, ascending=False)
    top_model = sorted_scores.iloc[0, sorted_scores.columns.str.contains('params')]
    top_model.index = top_model.index.str.replace('^params_', '', regex=True)

    if top_model.index.str.contains('\.').any():
        param_types = top_model.index.str.replace('\..+$', '', regex=True)
        param_names = top_model.index.str.replace('^.+\.', '', regex=True)
        param_values = top_model.values
        params = {}
        for ptype, pname, value in zip(param_types, param_names, param_values):
            argname = ptype + '_kwargs'
            if argname not in params:
                params[argname] = {}
            params[argname][pname] = value.item()

        params['training_kwargs']['batch_size'] = int(params['training_kwargs']['batch_size'])
        params['training_kwargs']['num_epochs'] = 50
        params['model_kwargs']['entity_initializer'] = xavier_normal_
        params['model_kwargs']['relation_initializer'] = xavier_normal_
        params['negative_sampler_kwargs'] = {}
        params['negative_sampler_kwargs']['num_negs_per_pos'] = 5

    else:
        param_names = top_model.index
        param_values = top_model.values
        params = {name: v.item() for name, v in zip(param_names, param_values)}

    return params


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('json', type=str, help='Path to graph json file, generated by Neo4j')
    parser.add_argument('-o', '--outdir', type=str, default=None, help='Filepath where model output should be saved')
    parser.add_argument('-t', '--test_size', type=float, default=0.1, help='Proportion of facts allocated to test set')
    parser.add_argument('-v', '--validation_size', type=float, default=0.1,
                        help='Proportion of facts allocated to validation set')
    args = parser.parse_args()

    custom_trials_path = 'data/hyperparams/custom_trials.tsv'

    custom_outpath = 'custom_model.p'

    gpus = tf.config.list_physical_devices('GPU')

    custom_hp_trials = pd.read_csv(custom_trials_path, sep='\t')
    custom_best_params = extract_best_params(custom_hp_trials, score_col='value')

    # separate learning rate from other params, make optimizer
    custom_best_model_params = {name: value for name, value in custom_best_params.items()
                                if name != 'lr'}
    lr = custom_best_params['lr']
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)

    # set seed
    tf.random.set_seed(123)

    model = CustomEmbeddingModel(args.json, state = 123)

    # train/test split
    model.split(model.embedding_data,
                test_size=args.test_size,
                validation_size=args.validation_size,
                stratify_column='specific_relation_lab',
                state=123)

    # initialize model with best params
    model.initializeModel(model.embedding_facts, **custom_best_model_params, optimizer=optimizer)

    train, test = model.final_train_set, model.test_set

    # get positive facts
    negs_per_pos = 5
    pos_facts = train[['head', 'general_relation', 'specific_relation', 'tail']].to_numpy(dtype='int32')

    # get batches
    batches = model.getBatches(pos_facts)

    # train the model and calculate AMRI
    if gpus:
        with tf.device('/GPU:0'):
            custom_results = model.train(batches, return_results=True)
            amri = model.AMRIEvaluation(test)
    else:
        custom_results = model.train(batches, return_results=True)
        amri = model.AMRIEvaluation(test)

    custom_results['params'] = custom_best_params
    custom_results['amri'] = amri
    print('AMRI: {}'.format(amri))

    # calculate AMRI for pathway relations, TF binding relations, and GO gene-term/term-term relations
    test_pathway_relations = test[test['general_relation_lab'] != test['specific_relation_lab']]
    amri_pathway_relations = model.AMRIEvaluation(test_pathway_relations)
    custom_results['amri_pathway_relations'] = amri_pathway_relations

    test_TF_relations = test[test['general_relation_lab'] == 'TF_BINDING']
    amri_TF_relations = model.AMRIEvaluation(test_TF_relations)
    custom_results['amri_TF_relations'] = amri_TF_relations

    test_GO_gene_relations = test[np.logical_xor(test['head_type'] == 'GeneOntologyTerm',
                                                 test['tail_type'] == 'GeneOntologyTerm')]
    amri_GO_gene_relations = model.AMRIEvaluation(test_GO_gene_relations)
    custom_results['amri_GO_gene_term_relations'] = amri_GO_gene_relations

    test_GO_term_relations = test[(test['head_type'] == 'GeneOntologyTerm') & (test['tail_type'] == 'GeneOntologyTerm')]
    amri_GO_term_relations = model.AMRIEvaluation(test_GO_term_relations)
    custom_results['amri_GO_term_term_relations'] = amri_GO_term_relations

    test_physint_relations = test[test['general_relation_lab'] == 'PHYSICAL_INTERACTION']
    amri_physint_relations = model.AMRIEvaluation(test_physint_relations)
    custom_results['amri_physical_int_relations'] = amri_physint_relations

    if args.outdir is not None:
        outdir = args.outdir + '/' if args.outdir[-1] != '/' else args.outdir
    else:
        outdir = ''

    # save pickle
    pickle.dump(custom_results, open(outdir + custom_outpath, 'wb'))

    custom_train_outpath = 'custom_train.tsv'
    custom_test_outpath = 'custom_test.tsv'

    # save train/test sets
    train.to_csv(outdir + custom_train_outpath, sep='\t', index=False)
    test.to_csv(outdir + custom_test_outpath, sep='\t', index=False)

